{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = \"Big cats are nice and funny.\"\n",
    "doc1 = \"Small dogs are better than big dogs.\"\n",
    "doc2 = \"Small cats are afraid of small dogs.\"\n",
    "doc3 = \"Big cats are not afraid of small dogs\"\n",
    "doc4 = \"Funny cats are not afraid of small dogs\"\n",
    "corpus = [doc0, doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "punctuation = \"[\"+punctuation+\"]\"\n",
    "\n",
    "#Remove punctuation \n",
    "def remove_punctuation(string) :\n",
    "    return(re.sub(punctuation, \"\", string))\n",
    "\n",
    "#Remove stopwords\n",
    "def remove_stopwords(tokens) :\n",
    "    without_stopwords = []\n",
    "    for token in tokens :\n",
    "        if token not in stopwords :\n",
    "            without_stopwords.append(token.lower())\n",
    "    return without_stopwords\n",
    "\n",
    "#Lemmatize\n",
    "def lemmatize(tokens) :\n",
    "    lemmas = []\n",
    "    pos = pos_tag(tokens)\n",
    "    for word, tag in pos :\n",
    "        tag_starting = tag[0].lower()\n",
    "        tag_starting = tag_starting if tag_starting in ['a', 'r', 'n', 'v'] else None\n",
    "        if not tag_starting :\n",
    "            lemma = word\n",
    "        else :\n",
    "            lemma = lemmatizer.lemmatize(word, tag_starting)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'cats', 'are', 'nice', 'and', 'funny']\n",
      "['small', 'dogs', 'are', 'better', 'than', 'big', 'dogs']\n",
      "['small', 'cats', 'are', 'afraid', 'of', 'small', 'dogs']\n",
      "['big', 'cats', 'are', 'not', 'afraid', 'of', 'small', 'dogs']\n",
      "['funny', 'cats', 'are', 'not', 'afraid', 'of', 'small', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "#Dictionary of document:[words]\n",
    "doc_terms = {}\n",
    "#Getting terms in each document(Removing spacing)\n",
    "doc_no = 0\n",
    "for doc in corpus :\n",
    "    doc_terms[doc_no] = []\n",
    "    doc = remove_punctuation(doc.lower())\n",
    "    doc_terms[doc_no]= doc.split()\n",
    "    print(doc_terms[doc_no])\n",
    "    doc_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['big', 'cat', 'nice', 'funny'], 1: ['small', 'dog', 'well', 'big', 'dog'], 2: ['small', 'cat', 'afraid', 'small', 'dog'], 3: ['big', 'cat', 'afraid', 'small', 'dog'], 4: ['funny', 'cat', 'afraid', 'small', 'dog']}\n",
      "['funny', 'dog', 'cat', 'big', 'afraid', 'well', 'nice', 'small']\n"
     ]
    }
   ],
   "source": [
    "indexes = []\n",
    "for doc, terms in doc_terms.items() :\n",
    "    terms = remove_stopwords(terms)\n",
    "    #print(doc_terms[doc])\n",
    "    terms = lemmatize(terms)\n",
    "    doc_terms[doc] = terms\n",
    "    #print(doc_terms[doc])\n",
    "    indexes.extend(terms) \n",
    "indexes = list(set(indexes))\n",
    "\n",
    "print(doc_terms)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Incidence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funny : [1 0 0 0 1]\n",
      "dog : [0 1 1 1 1]\n",
      "cat : [1 0 1 1 1]\n",
      "big : [1 1 0 1 0]\n",
      "afraid : [0 0 1 1 1]\n",
      "well : [0 1 0 0 0]\n",
      "nice : [1 0 0 0 0]\n",
      "small : [0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "term_incidence = {}\n",
    "for term in indexes :\n",
    "    term_incidence[term] = np.array([0]*len(corpus))\n",
    "    for i in range(len(corpus)) :\n",
    "        if term in doc_terms[i] :\n",
    "            term_incidence[term][i] = 1\n",
    "\n",
    "for term, incidence_vector in term_incidence.items() :\n",
    "    print(term,\":\", incidence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def And(top1, top2) :\n",
    "    res = []\n",
    "    for i in range(len(top1)):\n",
    "        if(top1[i] == 0 or top2[i] == 0) :\n",
    "            res.append(0)\n",
    "        else :\n",
    "            res.append(1)\n",
    "    return res\n",
    "\n",
    "def Or(top1, top2) :\n",
    "    res = []\n",
    "    for i in range(len(top1)):\n",
    "        if(top1[i] == 1 or top2[i] == 1) :\n",
    "            res.append(1)\n",
    "        else :\n",
    "            res.append(0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATORS = set([\"and\", \"or\", \"not\", \"(\", \")\"])\n",
    "PRIORITY = {\"and\":1, \"or\":1, \"not\":2}\n",
    "\n",
    "def infix_to_postfix(query):\n",
    "    output = []\n",
    "    stack = []\n",
    "    \n",
    "    for term in query :\n",
    "        if term not in OPERATORS :\n",
    "            output.append(term)\n",
    "        elif(term == \"(\") :\n",
    "            stack.append(term)\n",
    "        elif(term == \")\") :\n",
    "            while stack and stack[-1]!= '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()\n",
    "        else :\n",
    "            while stack and stack[-1]!='(' and PRIORITY[term]<=PRIORITY[stack[-1]]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(term)\n",
    "    while stack: \n",
    "        output.append(stack.pop())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'not', 'dog', 'and', 'dog', 'and']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infix_to_postfix(\"( not cat and dog ) and dog\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synonyms(word) :\n",
    "    synonyms = []\n",
    "    for ss in wn.synsets(word) :\n",
    "        for lemma in ss.lemmas() :\n",
    "            synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\"not\", \"and\", \"or\"]\n",
    "def query_processing(query) :\n",
    "    stack = []\n",
    "    postfix = infix_to_postfix(query)\n",
    "    if(not set(postfix).intersection(operations)) :\n",
    "        postfix.extend([\"and\"]*(len(postfix)-1))\n",
    "    print(postfix)\n",
    "    for term in postfix :\n",
    "        #print(stack)\n",
    "        if term not in operations :\n",
    "            if term in indexes :\n",
    "                stack.append(list(term_incidence[term]))\n",
    "            elif set(lemmatize(synonyms(term))).intersection(indexes):\n",
    "                term = list(set(lemmatize(synonyms(term))).intersection(indexes))[0]\n",
    "                stack.append(list(term_incidence[term]))\n",
    "            else :\n",
    "                stack.append([0]*len(corpus))\n",
    "        else :\n",
    "            if(term == \"not\") :\n",
    "                if(len(stack) >= 1) :\n",
    "                    top = stack.pop()\n",
    "                    top = [not i for i in top]\n",
    "                    #print(\"not\", top)\n",
    "                    stack.append([1 if i else 0 for i in top])\n",
    "                else :\n",
    "                    return []\n",
    "            elif(term == \"and\") :\n",
    "                if(len(stack) >= 2) :\n",
    "                    top1 = stack.pop()\n",
    "                    top2 = stack.pop()\n",
    "                    #print(\"and\", top1, top2)\n",
    "                    stack.append(And(top1, top2))\n",
    "                else :\n",
    "                    return []\n",
    "            elif(term == \"or\") :\n",
    "                if(len(stack) >= 2) :\n",
    "                    top1 = stack.pop()\n",
    "                    top2 = stack.pop()\n",
    "                    #print(\"or\", top1, top2)\n",
    "                    stack.append(Or(top1, top2))\n",
    "                else :\n",
    "                    return []\n",
    "    if(len(stack) != 1) :\n",
    "        return []\n",
    "    return stack[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query() :\n",
    "    query = input()\n",
    "    query = query.lower()\n",
    "    #query = remove_stopwords(query.split())\n",
    "    query = lemmatize(query.split())\n",
    "    #print(query)\n",
    "    common_terms = list(set(query).intersection(indexes))\n",
    "    for term in query :\n",
    "        common_terms.extend(set(lemmatize(synonyms(term))).intersection(indexes))\n",
    "    if(common_terms) :\n",
    "        if(len(query) == 1) :\n",
    "            relevance_vector = term_incidence[query[0]]\n",
    "        else :\n",
    "            relevance_vector = query_processing(query)\n",
    "        print(relevance_vector)\n",
    "        if(len(relevance_vector)) :\n",
    "            print(\"Relevant documents :\")\n",
    "            i = 0\n",
    "            for relevance in relevance_vector :\n",
    "                if(relevance) :\n",
    "                    print(corpus[i])\n",
    "                i += 1\n",
    "        else :\n",
    "            print(\"Either type a valid boolean query, or without any of the operators\")\n",
    "            get_query()\n",
    "    else :\n",
    "        print(\"None of the query terms match the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shady', 'amusing', 'laughable', 'funny_story', 'fishy', 'comical', 'funny', 'singular', 'suspicious', 'curious', 'risible', 'rum', 'peculiar', 'suspect', 'queer', 'funny_remark', 'rummy', 'mirthful', 'good_story', 'comic', 'odd']\n",
      "['diminished', 'modest', 'minor', 'lowly', 'minuscule', 'small-scale', 'little', 'pocket-sized', 'low', 'humble', 'pocket-size', 'belittled', 'small']\n"
     ]
    }
   ],
   "source": [
    "syns = synonyms('funny')\n",
    "print(syns)\n",
    "syns = synonyms('small')\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funny and small\n",
      "['funny', 'small', 'and']\n",
      "[0, 0, 0, 0, 1]\n",
      "Relevant documents :\n",
      "Funny cats are not afraid of small dogs\n"
     ]
    }
   ],
   "source": [
    "get_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amusing and minuscule\n",
      "['amusing', 'minuscule', 'and']\n",
      "[0, 0, 0, 0, 1]\n",
      "Relevant documents :\n",
      "Funny cats are not afraid of small dogs\n"
     ]
    }
   ],
   "source": [
    "get_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( not cat and dog ) and dog\n",
      "['cat', 'not', 'dog', 'and', 'dog', 'and']\n",
      "[0, 1, 0, 0, 0]\n",
      "Relevant documents :\n",
      "Small dogs are better than big dogs.\n"
     ]
    }
   ],
   "source": [
    "get_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
